# Visual Similarity Search with Metric Learning on Stanford Online Products

## Introduction

This project builds an instance-level visual similarity (image retrieval) system using deep metric learning. Starting from a pretrained ConvNeXt backbone, I fine-tuned the model using supervised contrastive learning on the Stanford Online Products (SOP) dataset, resulting in a significant improvement in retrieval performance measured by Recall@K.

The project covers the full pipeline:

- Dataset understanding and preprocessing
- Embedding extraction
- Metric learning fine-tuning
- FAISS-based nearest-neighbor retrieval
- Quantitative evaluation of retrieval
- Qualitative (visual) error analysis

## Motivation

Image retrieval problems arise in many real-world applications, such as:

- Product and spare-parts matching
- Visual search in e-commerce
- Deduplication and catalog cleaning
- Tool or component identification from images

Rather than classification, these tasks require learning a good embedding space where visually similar items are close together. This project demonstrates how metric learning can significantly improve retrieval quality over off-the-shelf pretrained embeddings.

## Dataset

I used the [Stanford Online Products (SOP) dataset](https://service.tib.eu/ldmservice/dataset/stanford-online-products), a standard benchmark for metric learning. Some of the characteristics of the data include:

- 120,053 images (split into ~59,551 train and ~60,502 test set)
- 22,634 product instances (`class_id`)
- 12 high-level categories (`super_class_id`)

The focus in this project is the `class_id`. Each product instance (`class_id`) has multiple images taken from different viewpoints and conditions. Many instances have only 2 images, making the task challenging. Below is the plot of the number of images per `class_id`.

![Number of images per class_id](/Users/samuel.omole/Desktop/repos/visual_similarity_sop/plots/images_per_class_id.png)

Dataset reference:

> Song et al., Deep Metric Learning via Lifted Structured Feature Embedding, CVPR 2016

## Method Overview

I started by using the pretrained `ConvNeXt-Base` model to generate emebeddings followed by retrieval using FAISS and evaluating retrieval results. The model backbone was then fine-tuned using the train set while adding a projection head that outputs 256-dimensional embeddings. For fine-tuning, I initially implemented supervised contrastive (SupCon) loss from scratch but then migrated to `pytorch-metric-learning` for reliability and extensibility. The SupCon loss optimises the separation of embeddings by pulling embeddings belonging to the same class closer together while pushing samples from different classes further apart.

### Baseline

- Backbone: ConvNeXt-Base (ImageNet pretrained, via timm)
- Embeddings extracted without task-specific fine-tuning
- Retrieval performed using FAISS with cosine similarity

### Fine-tuned Model

- Backbone: ConvNeXt-Base
- Projection head: 2-layer MLP → 256-dimensional embeddings
- Loss: Supervised Contrastive Loss (pytorch-metric-learning)
- Sampling: PK sampling (P classes × K images per class)
- Optimization: AdamW
- Early stopping based on validation Recall@1
- Embeddings extracted, retrieval performed and evaluated

### Retrieval

- Embeddings are L2-normalized
- FAISS IndexFlatIP (inner product = cosine similarity)
- Evaluation with Recall@1 / Recall@5 / Recall@10

## Results

The results include a comparison of the pretrained model with and without fine-tuning on the train set. In the section, the embeddings are visualised, the retrieval performance is evaluated and errors are analysed.

### Generated Embeddings

The embeddings generated by the pretrained model, without fine-tuning, are visualised here in 2D to show the clusters of image samples where classes have been grouped together in the embeddings space. In addition, the 3D space

<p align="center">
  <img src="./plots/embeddings_pretrained_model_2D.png" alt="Pretrained Model Embeddings (2D)" width="300">
  <br>
  <em>t-SNE projection of pretrained ConvNeXt embeddings colored by super_class (2D).</em>
</p>

<p align="center">
  <img src="./plots/embeddings_pretrained_model_3D.png" alt="Pretrained Model Embeddings (3D)" width="300">
  <br>
  <em>t-SNE projection of pretrained ConvNeXt embeddings colored by super_class (3D).</em>
</p>

Similarly, the embeddings generated by the fine-tuned model are shown

<p align="center">
  <img src="./plots/embeddings_finetuned_model_2D.png" alt="Finetuned Model Embeddings" width="300">
  <br>
  <em>t-SNE projection of finetuned ConvNeXt embeddings colored by super_class (2D).</em>
</p>

<p align="center">
  <img src="./plots/embeddings_finetuned_model_3D.png" alt="Finetuned Model Embeddings" width="300">
  <br>
  <em>t-SNE projection of finetuned ConvNeXt embeddings colored by super_class (3D).</em>
</p>

### Validation Performance (during training)

- Best validation Recall@1: 0.84
- Validation split created by holding out entire product instances (no class leakage)

### Within-test Retrieval Performance

The retrieval performance within the test set is shown for both the pretrained and fine-tuned models.

| Model                                | Recall@1 | Recall@5 | Recall@10 |
| ------------------------------------ | -------- | -------- | --------- |
| Pretrained ConvNeXt (no fine-tuning) | 0.59     | 0.70     | 0.74      |
| Fine-tuned (SupCon + PK sampling)    | 0.70     | 0.82     | 0.86      |

---

The improvement shows that supervised contrastive fine-tuning substantially improves the quality of the learned embedding space for retrieval. The following gains were made as shown in the table: Recall@1 has improved by ~19% (59% → 70%), Recall@5 by ~17% (70% → 82%) and Recall@10 by ~16% (74% → 86%).

### Visual Error Analysis

Quantitative metrics alone do not fully explain model behavior, so I performed qualitative error analysis by visualizing successful and failed retrievals.

Using `visualise_retrievals.py`, I did the following:

- Inspected top-K retrievals for randomly sampled queries in the test set
- Compared baseline vs fine-tuned models
- Saved failures and successes as structured artifacts (Parquet)

Some randomly selected top-K successes and failures are shown:
